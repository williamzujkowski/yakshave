# Real GitHub Data Testing Strategy for gh-year-end Website

## Executive Summary

This document proposes a three-tier strategy for testing the gh-year-end website with real GitHub data instead of synthetic test data. The recommended approach combines cached snapshots of real data with minimal API usage for flexibility.

## Current State Analysis

### Existing Test Infrastructure

1. **Synthetic Data**: Tests use `tests/fixtures/sample_org/` with hand-crafted JSONL files
   - 5 repos, 8 users, 20 PRs, 10 issues, 15 reviews
   - Deterministic and fully controlled
   - Generated by `fixture_generator.py`

2. **Data Collection Pipeline**: Collects via GitHub API
   - Repos, PRs, issues, reviews, comments, commits
   - Hygiene data (branch protection, security features, file presence)
   - Rate-limited with checkpoint/resume support

3. **Available User Repos**: You have ~10+ public repos including:
   - `williamzujkowski/yakshave` (this project, actively developed)
   - `williamzujkowski/williamzujkowski` (profile repo)
   - `williamzujkowski/cognitive-toolworks`
   - Various other projects

### Key Requirements

1. Realistic data for all metric types
2. Minimal API calls (respect rate limits)
3. Fast setup for iterative testing
4. Reproducible results
5. Works offline after initial collection

## Recommended Strategy: Three-Tier Approach

### Tier 1: Cached Real Data Snapshot (PRIMARY - RECOMMENDED)

**Create a permanent fixture from a one-time real data collection.**

#### Implementation

```bash
# One-time collection script
scripts/capture_real_fixture.sh
```

**What it does:**
1. Collect real data from `williamzujkowski` user account (2024 data)
2. Save to `tests/fixtures/real_williamzujkowski_2024/`
3. Commit this snapshot to the repository
4. Tests use this cached data with zero API calls

**Advantages:**
- No API calls during testing
- Fast and deterministic
- Realistic data patterns
- Works offline
- Can be version controlled
- Reviewers can verify against real data

**Disadvantages:**
- Static snapshot (becomes stale over time)
- Need to manually refresh periodically
- Large fixture files (~1-5 MB)

#### Recommended Configuration

```yaml
# tests/fixtures/real_williamzujkowski_2024_config.yaml
github:
  target:
    mode: user
    name: williamzujkowski
  discovery:
    include_forks: false
    include_archived: true  # For realistic diversity
    visibility: public
  windows:
    year: 2024
    since: "2024-01-01T00:00:00Z"
    until: "2025-01-01T00:00:00Z"

collection:
  enable:
    pulls: true
    issues: true
    reviews: true
    comments: true
    commits: true
    hygiene: true
```

**Estimated Data Volume:**
- Based on your repos: ~10 repos
- Expected: 20-50 PRs, 10-30 issues, 15-40 reviews
- Storage: ~2-3 MB of JSONL files

### Tier 2: Minimal Live Collection Script (DEVELOPMENT)

**For developers who want fresh data during development.**

#### Implementation

```bash
# scripts/dev_collect_minimal.sh --repo williamzujkowski/yakshave
```

**What it does:**
1. Accept a single repo as input
2. Collect last 30 days of activity
3. Save to local developer's `data/dev/` (gitignored)
4. Developer can test against this fresh data

**Advantages:**
- Fresh, current data
- Single repo = minimal API calls (~10-50 requests)
- Flexible for testing specific scenarios
- Can test against any accessible repo

**Disadvantages:**
- Requires GitHub token
- Uses API quota
- Slower than cached fixtures
- Results vary over time

#### Recommended Usage

```bash
# Quick test against yakshave repo (small, active)
uv run python scripts/dev_collect_minimal.py \
  --repo williamzujkowski/yakshave \
  --days 30 \
  --output data/dev/

# Test the report
uv run gh-year-end all --config config/dev_minimal.yaml
```

### Tier 3: Small Well-Known OSS Project (ALTERNATIVE)

**Use a small, stable open-source project as a reference dataset.**

#### Recommended Projects

| Project | Why | Characteristics |
|---------|-----|----------------|
| `pytest-dev/pytest` | Well-maintained, stable | ~200 PRs/year, active community |
| `d3/d3` | Relevant to project (D3.js used in site) | Moderate activity, good diversity |
| `pallets/flask` | Popular Python project | Good mix of contributors |
| **williamzujkowski/yakshave** | Your own project (BEST) | Complete control, relevant data |

**Advantages:**
- Publicly accessible
- Realistic patterns
- Stable reference point
- Community can reproduce

**Disadvantages:**
- More API calls than user data
- Need to pick appropriate size
- May include data you don't control

## Detailed Implementation Plan

### Phase 1: Create Cached Real Data Fixture (1-2 hours)

1. **Create collection script**

```bash
#!/usr/bin/env bash
# scripts/capture_real_fixture.sh

set -e

echo "Capturing real data fixture from williamzujkowski (2024)"

# Create fixture directory
FIXTURE_DIR="tests/fixtures/real_williamzujkowski_2024"
mkdir -p "$FIXTURE_DIR"

# Run collection with minimal config
uv run gh-year-end collect \
  --config tests/fixtures/real_williamzujkowski_2024_config.yaml \
  --force

# Copy collected data to fixtures
cp -r data/raw "$FIXTURE_DIR/"

# Create README
cat > "$FIXTURE_DIR/README.md" << 'EOF'
# Real Data Fixture: williamzujkowski 2024

**Captured**: $(date -I)
**Source**: williamzujkowski GitHub user account
**Period**: 2024-01-01 to 2025-01-01

## Contents

- Repositories: ~10 public repos
- Pull Requests: Real PRs from 2024
- Issues: Real issues from 2024
- Reviews: Real code reviews
- Commits: Real commit history
- Hygiene: Real repo health data

## Refresh

To refresh this fixture with new data:

```bash
./scripts/capture_real_fixture.sh
```

**Last Updated**: $(date -I)
EOF

echo "✓ Fixture captured to $FIXTURE_DIR"
echo "Total size: $(du -sh $FIXTURE_DIR)"
```

2. **Create test configuration**

```yaml
# tests/fixtures/real_williamzujkowski_2024_config.yaml
github:
  target:
    mode: user
    name: williamzujkowski
  auth:
    token_env: GITHUB_TOKEN
  discovery:
    include_forks: false
    include_archived: true
    visibility: public
  windows:
    year: 2024
    since: "2024-01-01T00:00:00Z"
    until: "2025-01-01T00:00:00Z"

rate_limit:
  strategy: adaptive
  max_concurrency: 2
  min_sleep_seconds: 1
  max_sleep_seconds: 60

identity:
  bots:
    exclude_patterns:
      - ".*\\[bot\\]$"
      - "^dependabot$"
      - "^renovate\\[bot\\]$"
    include_overrides: []
  humans_only: true

collection:
  enable:
    pulls: true
    issues: true
    reviews: true
    comments: true
    commits: true
    hygiene: true

storage:
  root: "tests/fixtures/real_williamzujkowski_2024/data"
```

3. **Run one-time collection**

```bash
# Set your GitHub token
export GITHUB_TOKEN=ghp_xxxxx

# Capture the fixture
./scripts/capture_real_fixture.sh

# Expected time: 2-5 minutes
# Expected API calls: 50-200 requests
```

4. **Create test using real fixture**

```python
# tests/test_real_data_smoke.py
"""Smoke tests using real GitHub data fixtures.

Uses cached real data from williamzujkowski user account (2024).
No API calls required - tests run offline.
"""

from pathlib import Path
import pytest
from gh_year_end.config import Config
from gh_year_end.storage.paths import PathManager

REAL_FIXTURES_DIR = Path(__file__).parent / "fixtures" / "real_williamzujkowski_2024"

@pytest.fixture
def real_config(tmp_path: Path) -> Config:
    """Config using real cached data."""
    # Copy fixtures to temp to avoid modifying originals
    import shutil
    fixture_raw = REAL_FIXTURES_DIR / "raw"
    temp_raw = tmp_path / "data" / "raw"
    shutil.copytree(fixture_raw, temp_raw)

    return Config.model_validate({
        "github": {
            "target": {"mode": "user", "name": "williamzujkowski"},
            "windows": {
                "year": 2024,
                "since": "2024-01-01T00:00:00Z",
                "until": "2025-01-01T00:00:00Z",
            },
        },
        "storage": {"root": str(tmp_path / "data")},
        "report": {
            "title": "williamzujkowski 2024 Year in Review",
            "output_dir": str(tmp_path / "site"),
        },
        "identity": {
            "bots": {
                "exclude_patterns": [r".*\[bot\]$"],
            }
        },
    })

class TestRealDataPipeline:
    """Test pipeline with real GitHub data."""

    def test_real_data_exists(self) -> None:
        """Verify real fixture data exists."""
        repos_file = REAL_FIXTURES_DIR / "raw" / "year=2024" / "source=github" / "target=williamzujkowski" / "repos.jsonl"
        assert repos_file.exists(), "Real repos fixture should exist"

    def test_normalize_real_data(self, real_config: Config) -> None:
        """Test normalization with real data."""
        paths = PathManager(real_config)

        # Run normalization
        from gh_year_end.normalize.repos import normalize_repos
        df = normalize_repos(paths.raw_root, real_config)

        # Real data should have repos
        assert len(df) > 0, "Should normalize real repos"

    def test_full_pipeline_real_data(self, real_config: Config) -> None:
        """Test complete pipeline with real data."""
        # This runs normalize -> metrics -> report
        # with zero API calls (uses cached data)
        paths = PathManager(real_config)

        # Would run full pipeline here
        # assert (paths.site_root / "index.html").exists()
```

### Phase 2: Create Minimal Live Collection Script (Optional)

```python
#!/usr/bin/env python3
# scripts/dev_collect_minimal.py
"""Minimal real data collector for development testing.

Collects recent activity from a single repo for quick testing.
Uses minimal API calls - suitable for frequent use during development.

Usage:
    python scripts/dev_collect_minimal.py --repo williamzujkowski/yakshave --days 30
"""

import argparse
import os
import sys
from datetime import datetime, timedelta, UTC
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from gh_year_end.config import Config
from gh_year_end.github.auth import GitHubAuth
from gh_year_end.github.http import GitHubClient
from gh_year_end.github.rest import RestClient
from gh_year_end.github.ratelimit import AdaptiveRateLimiter

async def collect_minimal(repo_full_name: str, days: int, output_dir: Path):
    """Collect minimal data from a single repo.

    Args:
        repo_full_name: Repository in format "owner/repo"
        days: Number of days of history to collect
        output_dir: Where to save collected data
    """
    print(f"Collecting {days} days of data from {repo_full_name}")

    # Calculate date range
    until = datetime.now(UTC)
    since = until - timedelta(days=days)

    # Create minimal config
    config = Config.model_validate({
        "github": {
            "target": {"mode": "repo", "name": repo_full_name},
            "windows": {
                "year": until.year,
                "since": since.isoformat(),
                "until": until.isoformat(),
            },
        },
        "storage": {"root": str(output_dir)},
        "collection": {
            "enable": {
                "pulls": True,
                "issues": True,
                "reviews": True,
                "comments": False,  # Skip for speed
                "commits": False,   # Skip for speed
                "hygiene": True,
            }
        }
    })

    # Collect data
    # ... (implementation would call collection pipeline)

    print(f"✓ Data collected to {output_dir}")
    print(f"  API calls used: ~20-50")
    print(f"  Time elapsed: ~30-60 seconds")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Minimal real data collector")
    parser.add_argument("--repo", required=True, help="Repository (owner/repo)")
    parser.add_argument("--days", type=int, default=30, help="Days of history")
    parser.add_argument("--output", default="data/dev", help="Output directory")

    args = parser.parse_args()

    import asyncio
    asyncio.run(collect_minimal(
        args.repo,
        args.days,
        Path(args.output)
    ))
```

## Comparison Matrix

| Approach | API Calls | Speed | Freshness | Offline | Best For |
|----------|-----------|-------|-----------|---------|----------|
| **Cached Snapshot (Tier 1)** | 0 (after capture) | Instant | Static | ✓ | CI/CD, PR reviews, most testing |
| **Minimal Live (Tier 2)** | ~20-50 | 30-60s | Fresh | ✗ | Development, debugging specific issues |
| **Full User Collection** | ~200-500 | 2-5 min | Fresh | ✗ | Refreshing fixtures, initial setup |
| **Synthetic Data** | 0 | Instant | N/A | ✓ | Unit tests, edge cases |

## Recommended Testing Workflow

### For Most Developers (Day-to-Day)

```bash
# Use cached real data - zero API calls
uv run pytest tests/test_real_data_smoke.py

# Or run full pipeline with cached data
uv run gh-year-end all --config tests/fixtures/real_williamzujkowski_2024_config.yaml
```

### For Website Development

```bash
# Normalize cached real data
uv run gh-year-end normalize --config tests/fixtures/real_williamzujkowski_2024_config.yaml

# Generate metrics
uv run gh-year-end metrics --config tests/fixtures/real_williamzujkowski_2024_config.yaml

# Build site
uv run gh-year-end report --config tests/fixtures/real_williamzujkowski_2024_config.yaml

# View site
python -m http.server -d tests/fixtures/real_williamzujkowski_2024/data/../../../site/2024
```

### For Fixture Maintenance (Quarterly)

```bash
# Refresh cached fixture with latest data
./scripts/capture_real_fixture.sh

# Commit updated fixture
git add tests/fixtures/real_williamzujkowski_2024/
git commit -m "chore: refresh real data fixture (2024 Q4)"
```

## Implementation Priority

### Immediate (Week 1)
1. Create `scripts/capture_real_fixture.sh`
2. Run one-time collection for williamzujkowski 2024 data
3. Commit cached fixture to repository
4. Create test using cached data

### Short-term (Week 2-3)
1. Add fixture to CI/CD pipeline
2. Document fixture refresh process
3. Create comparison tests (synthetic vs real data)

### Optional (Future)
1. Create minimal live collection script
2. Add support for specifying custom repos
3. Automate quarterly fixture refresh

## Tradeoffs Analysis

### Why Cached Snapshot is Best

1. **Zero API Cost**: No rate limit concerns
2. **Fast**: Tests complete in seconds
3. **Reproducible**: Same data every time
4. **Offline**: Works without network
5. **CI-Friendly**: No secrets required in CI
6. **Reviewable**: Others can verify against real data

### When to Use Live Collection

1. Testing API changes
2. Debugging rate limiting
3. Validating against latest GitHub API
4. Collecting from private repos
5. One-time fixture generation

### Why Not Full OSS Project

1. More API calls than user data
2. Don't control the data source
3. Larger dataset = slower tests
4. May have unexpected patterns

## Estimated Resource Usage

### Initial Fixture Capture
- **Time**: 2-5 minutes
- **API Calls**: 150-300 requests
- **Storage**: 2-5 MB
- **Rate Limit Impact**: Minimal (< 5% of hourly limit)

### Using Cached Fixture
- **Time**: < 1 second
- **API Calls**: 0
- **Storage**: 2-5 MB (committed to repo)
- **Rate Limit Impact**: None

### Minimal Live Collection (per run)
- **Time**: 30-60 seconds
- **API Calls**: 20-50 requests
- **Storage**: 200-500 KB
- **Rate Limit Impact**: Negligible

## Conclusion

**Recommendation: Implement Tier 1 (Cached Real Data Snapshot) immediately.**

This provides the best balance of:
- Realistic data
- Zero API usage during testing
- Fast, reproducible tests
- Offline capability
- Easy maintenance

Keep synthetic data for unit tests and edge cases. Use live collection only for development debugging or fixture refresh.

## Next Steps

1. Run the capture script to create the cached fixture
2. Add fixture to git repository
3. Create tests using the cached data
4. Document in main README
5. Add to CI/CD pipeline

Total implementation time: 2-4 hours
Total ongoing cost: 0 API calls, <5 MB storage
